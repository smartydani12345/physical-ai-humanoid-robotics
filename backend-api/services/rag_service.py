import asyncio
from typing import List, Dict, Any, Optional, AsyncGenerator
import logging
import cohere
from qdrant_client import QdrantClient
from qdrant_client.http import models
import google.generativeai as genai
import os
from dataclasses import dataclass

from services.database_service import DatabaseService
from config import settings

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class Document:
    id: str
    content: str
    metadata: Dict[str, Any]

class RAGService:
    def __init__(self):
        # Initialize database service for chat history persistence
        self.db_service = DatabaseService()

        # Qdrant collection name
        self.collection_name = "humanoid_ai_book"

        # Initialize clients as None - will be initialized when first used
        self._cohere_client = None
        self._qdrant_client = None

    @property
    def cohere_client(self):
        if self._cohere_client is None:
            self._cohere_client = cohere.Client(settings.cohere_api_key)
        return self._cohere_client

    @property
    def qdrant_client(self):
        if self._qdrant_client is None:
            self._qdrant_client = QdrantClient(
                url=settings.qdrant_url,
                api_key=settings.qdrant_api_key,
            )
        return self._qdrant_client

    def _get_gemini_client(self):
        """Initialize and return Gemini client"""
        genai.configure(api_key=settings.gemini_api_key)
        model = genai.GenerativeModel('gemini-pro')
        return model

    async def initialize_services(self):
        """Initialize dependent services like database connection"""
        await self.db_service.initialize()

    def _ensure_collection_exists(self):
        """Ensure the Qdrant collection exists with the correct configuration"""
        try:
            # Check if collection exists
            collections = self.qdrant_client.get_collections()
            collection_names = [collection.name for collection in collections.collections]

            if self.collection_name not in collection_names:
                # Create collection with specified payload schema
                self.qdrant_client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=models.VectorParams(
                        size=1024,  # Cohere embed v3 has 1024 dimensions
                        distance=models.Distance.COSINE
                    )
                )
                logger.info(f"Created Qdrant collection: {self.collection_name}")
            else:
                logger.info(f"Qdrant collection exists: {self.collection_name}")
        except Exception as e:
            logger.error(f"Error ensuring collection exists: {str(e)}")
            raise

    def _load_sample_content(self):
        """This method is no longer needed since we're using Qdrant Cloud"""
        # Content should be pre-loaded in Qdrant Cloud
        pass

    def _generate_embeddings(self):
        """This method is no longer needed since we're using Cohere embeddings with Qdrant"""
        # Embeddings are generated by Cohere and stored in Qdrant
        pass

    def _find_relevant_documents(self, query: str, top_k: int = 3) -> List[Document]:
        """Find the most relevant documents for a query using Qdrant semantic search"""
        try:
            # Generate embedding for the query using Cohere
            response = self.cohere_client.embed(
                texts=[query],
                model="embed-english-v3.0",
                input_type="search_query"
            )
            query_embedding = response.embeddings[0]

            # Query Qdrant for similar documents using the correct API for the installed version
            search_result = self.qdrant_client.query_points(
                collection_name=self.collection_name,
                query=query_embedding,
                limit=top_k,
                with_payload=True
            )

            # Convert search results to Document objects
            documents = []
            for result in search_result.points:
                documents.append(Document(
                    id=str(result.id),
                    content=result.payload.get("text", "") if result.payload else "",
                    metadata={
                        "chapter": result.payload.get("chapter", "") if result.payload else "",
                        "section": result.payload.get("section", "") if result.payload else "",
                        "source_url": result.payload.get("source_url", "") if result.payload else "",
                        "score": result.score
                    }
                ))

            return documents
        except Exception as e:
            logger.error(f"Error finding relevant documents: {str(e)}")
            return []

    async def process_query(self, query: str, selected_text: Optional[str] = None,
                           chat_history: Optional[List[Dict[str, str]]] = None,
                           context: Optional[Dict[str, Any]] = None,
                           conversation_id: Optional[str] = None) -> Dict[str, Any]:
        """Process a query using RAG methodology"""
        try:
            # If selected_text exists, answer ONLY from selected_text
            if selected_text:
                logger.info("Using selected_text for response generation")
                context_text = selected_text
                relevant_docs = []
            else:
                # Find relevant documents from Qdrant
                relevant_docs = self._find_relevant_documents(query)

                # If no relevant content found, respond with "I don't know"
                if not relevant_docs:
                    logger.info("No relevant content found in Qdrant")
                    result = {
                        "response": "I don't know",
                        "sources": [],
                        "context": {
                            "retrieved_docs": 0,
                            "query": query
                        }
                    }
                    # Store user query and response in database if conversation_id provided
                    if conversation_id:
                        await self.db_service.add_message(conversation_id, "user", query)
                        await self.db_service.add_message(conversation_id, "assistant", result["response"])
                    return result

                # Create context from relevant documents
                context_text = "\n".join([doc.content for doc in relevant_docs])

            # If conversation_id is provided, get history from database
            if conversation_id:
                db_history = await self.db_service.get_conversation_history(conversation_id)
                # Combine with any provided history
                if chat_history:
                    full_history = db_history + chat_history
                else:
                    full_history = db_history
            else:
                full_history = chat_history or []

            # Generate response using Gemini with the query and context
            response = await self._generate_response_with_gemini(query, context_text, full_history)

            # Extract sources from relevant documents
            sources = [doc.metadata.get("source_url", "") for doc in relevant_docs if doc.metadata.get("source_url")]

            result = {
                "response": response,
                "sources": sources,
                "context": {
                    "retrieved_docs": len(relevant_docs),
                    "query": query
                }
            }

            # Generate Urdu translation if needed
            urdu_translation = await self._translate_to_urdu(response) if response != "I don't know" and response != "I'm sorry, I encountered an error processing your request. Please try again." else None

            # Store user query and response in database if conversation_id provided
            if conversation_id:
                await self.db_service.add_message(conversation_id, "user", query)
                await self.db_service.add_message(conversation_id, "assistant", response, urdu_translation)

            return result
        except Exception as e:
            logger.error(f"Error processing query: {str(e)}")
            result = {
                "response": "I'm sorry, I encountered an error processing your request. Please try again.",
                "sources": [],
                "context": {}
            }
            # Store error response in database if conversation_id provided
            if conversation_id:
                await self.db_service.add_message(conversation_id, "user", query, None)
                await self.db_service.add_message(conversation_id, "assistant", result["response"], None)
            return result

    async def _translate_to_urdu(self, text: str) -> Optional[str]:
        """Translate text to Urdu using Gemini"""
        try:
            # Only translate if text is substantial
            if not text or len(text.strip()) < 5:
                return None

            model = self._get_gemini_client()

            prompt = f"Translate the following text to Urdu (اُردو). Only respond with the translation and nothing else:\n\n{text}"

            response = model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.3,
                    max_output_tokens=1000
                )
            )

            translation = response.text if response.text else None

            # Ensure we only return the translation, not additional text
            if translation and ".Translate" in translation:
                # If Gemini added extra text, try to extract just the translation
                if ":" in translation:
                    translation = translation.split(":", 1)[1].strip()

            return translation
        except Exception as e:
            logger.error(f"Error translating to Urdu: {str(e)}")
            return None

    async def _generate_response_with_gemini(self, query: str, context: str, chat_history: Optional[List[Dict[str, str]]] = None) -> str:
        """Generate response using Gemini based on query and context"""
        try:
            # Format chat history for context
            history_text = ""
            if chat_history:
                for msg in chat_history[-5:]:  # Use last 5 messages as context
                    role = msg.get('role', 'user')
                    content = msg.get('content', '')
                    history_text += f"{role.capitalize()}: {content}\n"

            # Prepare the prompt for Gemini
            prompt = f"""
            You are an academic tutor specializing in Physical AI & Humanoid Robotics.
            Answer the following question based on the provided context in an academic, concise, and tutor-style manner.

            Conversation History:
            {history_text}

            Context: {context}

            Question: {query}

            Provide a clear, factual answer based only on the context provided.
            If the context doesn't contain the information needed to answer the question, respond with "I don't know".
            Do not hallucinate or make up information.
            """

            model = self._get_gemini_client()

            # Generate response using Gemini
            response = model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.7,
                    max_output_tokens=1000
                )
            )

            # Return the generated text
            return response.text if response.text else "I don't know"
        except Exception as e:
            logger.error(f"Error generating response with Gemini: {str(e)}")
            return "I don't know"

    async def process_query_stream(self, query: str, selected_text: Optional[str] = None,
                                  chat_history: Optional[List[Dict[str, str]]] = None,
                                  context: Optional[Dict[str, Any]] = None,
                                  conversation_id: Optional[str] = None) -> AsyncGenerator[str, None]:
        """Process a query and return response in streaming fashion"""
        try:
            # If selected_text exists, answer ONLY from selected_text
            if selected_text:
                logger.info("Using selected_text for streaming response generation")
                context_text = selected_text
                relevant_docs = []
            else:
                # Find relevant documents from Qdrant
                relevant_docs = self._find_relevant_documents(query)

                # If no relevant content found, respond with "I don't know"
                if not relevant_docs:
                    logger.info("No relevant content found in Qdrant")
                    response = "I don't know"
                    # Store user query and response in database if conversation_id provided
                    if conversation_id:
                        await self.db_service.add_message(conversation_id, "user", query)
                        await self.db_service.add_message(conversation_id, "assistant", response)
                    yield response
                    return

                # Create context from relevant documents
                context_text = "\n".join([doc.content for doc in relevant_docs])

            # If conversation_id is provided, get history from database
            if conversation_id:
                db_history = await self.db_service.get_conversation_history(conversation_id)
                # Combine with any provided history
                if chat_history:
                    full_history = db_history + chat_history
                else:
                    full_history = db_history
            else:
                full_history = chat_history or []

            # Format chat history for context
            history_text = ""
            if full_history:
                for msg in full_history[-5:]:  # Use last 5 messages as context
                    role = msg.get('role', 'user')
                    content = msg.get('content', '')
                    history_text += f"{role.capitalize()}: {content}\n"

            # Prepare the prompt for Gemini
            prompt = f"""
            You are an academic tutor specializing in Physical AI & Humanoid Robotics.
            Answer the following question based on the provided context in an academic, concise, and tutor-style manner.

            Conversation History:
            {history_text}

            Context: {context_text}

            Question: {query}

            Provide a clear, factual answer based only on the context provided.
            If the context doesn't contain the information needed to answer the question, respond with "I don't know".
            Do not hallucinate or make up information.
            """

            # For streaming with Gemini, we'll generate the full response and then simulate streaming
            # Note: The Google Generative AI library doesn't support true streaming in the same way
            model = self._get_gemini_client()

            response = model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.7,
                    max_output_tokens=1000
                ),
                stream=False  # Using non-streaming for compatibility
            )

            full_response = response.text if response.text else "I don't know"

            # Simulate streaming by yielding the response in chunks
            chunk_size = 50  # characters per chunk
            for i in range(0, len(full_response), chunk_size):
                chunk = full_response[i:i + chunk_size]
                yield chunk

            # Generate Urdu translation if needed
            urdu_translation = await self._translate_to_urdu(full_response) if full_response != "I don't know" and full_response != "I'm sorry, I encountered an error processing your request." else None

            # Store user query and response in database if conversation_id provided
            if conversation_id:
                await self.db_service.add_message(conversation_id, "user", query)
                await self.db_service.add_message(conversation_id, "assistant", full_response, urdu_translation)
        except Exception as e:
            logger.error(f"Error processing streaming query: {str(e)}")
            error_response = "I'm sorry, I encountered an error processing your request."
            # Store error response in database if conversation_id provided
            if conversation_id:
                await self.db_service.add_message(conversation_id, "user", query, None)
                await self.db_service.add_message(conversation_id, "assistant", error_response, None)
            yield error_response

    def health_check(self) -> bool:
        """Check if the RAG service is healthy"""
        try:
            # Test Qdrant connection by getting collection info
            collections = self.qdrant_client.get_collections()
            collection_names = [collection.name for collection in collections.collections]
            return self.collection_name in collection_names
        except:
            return False