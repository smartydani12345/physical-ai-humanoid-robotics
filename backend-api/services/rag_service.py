import asyncio
from typing import List, Dict, Any, Optional, AsyncGenerator
import logging
import cohere
from qdrant_client import QdrantClient
from qdrant_client.http import models
from openai import OpenAI
import os
from dataclasses import dataclass

from services.database_service import DatabaseService

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class Document:
    id: str
    content: str
    metadata: Dict[str, Any]

class RAGService:
    def __init__(self):
        # Initialize database service for chat history persistence
        self.db_service = DatabaseService()

        # Qdrant collection name
        self.collection_name = "humanoid_ai_book"

        # Initialize clients as None - will be initialized when first used
        self._cohere_client = None
        self._qdrant_client = None
        self._openai_client = None

    @property
    def cohere_client(self):
        if self._cohere_client is None:
            self._cohere_client = cohere.Client(os.getenv("COHERE_API_KEY"))
        return self._cohere_client

    @property
    def qdrant_client(self):
        if self._qdrant_client is None:
            self._qdrant_client = QdrantClient(
                url=os.getenv("QDRANT_URL"),
                api_key=os.getenv("QDRANT_API_KEY"),
            )
        return self._qdrant_client

    @property
    def openai_client(self):
        if self._openai_client is None:
            # Initialize OpenAI client with Gemini-compatible endpoint
            self._openai_client = OpenAI(
                api_key=os.getenv("GEMINI_API_KEY"),
                base_url="https://generativelanguage.googleapis.com/v1beta/openai/"  # Gemini OpenAI-compatible endpoint
            )
        return self._openai_client

    async def initialize_services(self):
        """Initialize dependent services like database connection"""
        await self.db_service.initialize()

    def _ensure_collection_exists(self):
        """Ensure the Qdrant collection exists with the correct configuration"""
        try:
            # Check if collection exists
            collections = self.qdrant_client.get_collections()
            collection_names = [collection.name for collection in collections.collections]

            if self.collection_name not in collection_names:
                # Create collection with specified payload schema
                self.qdrant_client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=models.VectorParams(
                        size=1024,  # Cohere embed v3 has 1024 dimensions
                        distance=models.Distance.COSINE
                    )
                )
                logger.info(f"Created Qdrant collection: {self.collection_name}")
            else:
                logger.info(f"Qdrant collection exists: {self.collection_name}")
        except Exception as e:
            logger.error(f"Error ensuring collection exists: {str(e)}")
            raise

    def _load_sample_content(self):
        """This method is no longer needed since we're using Qdrant Cloud"""
        # Content should be pre-loaded in Qdrant Cloud
        pass

    def _generate_embeddings(self):
        """This method is no longer needed since we're using Cohere embeddings with Qdrant"""
        # Embeddings are generated by Cohere and stored in Qdrant
        pass

    def _find_relevant_documents(self, query: str, top_k: int = 3) -> List[Document]:
        """Find the most relevant documents for a query using Qdrant semantic search"""
        try:
            # Generate embedding for the query using Cohere
            response = self.cohere_client.embed(
                texts=[query],
                model="embed-english-v3.0",
                input_type="search_query"
            )
            query_embedding = response.embeddings[0]

            # Search in Qdrant for similar documents
            search_result = self.qdrant_client.search(
                collection_name=self.collection_name,
                query_vector=query_embedding,
                limit=top_k,
                with_payload=True
            )

            # Convert search results to Document objects
            documents = []
            for result in search_result:
                documents.append(Document(
                    id=result.id,
                    content=result.payload.get("text", ""),
                    metadata={
                        "chapter": result.payload.get("chapter", ""),
                        "section": result.payload.get("section", ""),
                        "source_url": result.payload.get("source_url", ""),
                        "score": result.score
                    }
                ))

            return documents
        except Exception as e:
            logger.error(f"Error finding relevant documents: {str(e)}")
            return []

    async def process_query(self, query: str, selected_text: Optional[str] = None,
                           chat_history: Optional[List[Dict[str, str]]] = None,
                           context: Optional[Dict[str, Any]] = None,
                           conversation_id: Optional[str] = None) -> Dict[str, Any]:
        """Process a query using RAG methodology"""
        try:
            # If selected_text exists, answer ONLY from selected_text
            if selected_text:
                logger.info("Using selected_text for response generation")
                context_text = selected_text
                relevant_docs = []
            else:
                # Find relevant documents from Qdrant
                relevant_docs = self._find_relevant_documents(query)

                # If no relevant content found, respond with "I don't know"
                if not relevant_docs:
                    logger.info("No relevant content found in Qdrant")
                    result = {
                        "response": "I don't know",
                        "sources": [],
                        "context": {
                            "retrieved_docs": 0,
                            "query": query
                        }
                    }
                    # Store user query and response in database if conversation_id provided
                    if conversation_id:
                        await self.db_service.add_message(conversation_id, "user", query)
                        await self.db_service.add_message(conversation_id, "assistant", result["response"])
                    return result

                # Create context from relevant documents
                context_text = "\n".join([doc.content for doc in relevant_docs])

            # If conversation_id is provided, get history from database
            if conversation_id:
                db_history = await self.db_service.get_conversation_history(conversation_id)
                # Combine with any provided history
                if chat_history:
                    full_history = db_history + chat_history
                else:
                    full_history = db_history
            else:
                full_history = chat_history or []

            # Generate response using Gemini with the query and context
            response = await self._generate_response_with_gemini(query, context_text, full_history)

            # Extract sources from relevant documents
            sources = [doc.metadata.get("source_url", "") for doc in relevant_docs if doc.metadata.get("source_url")]

            result = {
                "response": response,
                "sources": sources,
                "context": {
                    "retrieved_docs": len(relevant_docs),
                    "query": query
                }
            }

            # Generate Urdu translation if needed
            urdu_translation = await self._translate_to_urdu(response) if response != "I don't know" and response != "I'm sorry, I encountered an error processing your request. Please try again." else None

            # Store user query and response in database if conversation_id provided
            if conversation_id:
                await self.db_service.add_message(conversation_id, "user", query)
                await self.db_service.add_message(conversation_id, "assistant", response, urdu_translation)

            return result
        except Exception as e:
            logger.error(f"Error processing query: {str(e)}")
            result = {
                "response": "I'm sorry, I encountered an error processing your request. Please try again.",
                "sources": [],
                "context": {}
            }
            # Store error response in database if conversation_id provided
            if conversation_id:
                await self.db_service.add_message(conversation_id, "user", query, None)
                await self.db_service.add_message(conversation_id, "assistant", result["response"], None)
            return result

    async def _translate_to_urdu(self, text: str) -> Optional[str]:
        """Translate text to Urdu using Gemini via OpenAI-compatible endpoint"""
        try:
            # Only translate if text is substantial
            if not text or len(text.strip()) < 5:
                return None

            prompt = f"Translate the following text to Urdu (اُردو). Only respond with the translation and nothing else:\n\n{text}"

            response = await self.openai_client.chat.completions.create(
                model="gemini-pro",
                messages=[
                    {"role": "system", "content": "You are a professional translator specializing in English to Urdu translation. Only provide the Urdu translation without any additional text or explanations."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=1000
            )

            translation = response.choices[0].message.content if response.choices[0].message.content else None

            # Ensure we only return the translation, not additional text
            if translation and ".Translate" in translation:
                # If Gemini added extra text, try to extract just the translation
                if ":" in translation:
                    translation = translation.split(":", 1)[1].strip()

            return translation
        except Exception as e:
            logger.error(f"Error translating to Urdu: {str(e)}")
            return None

    async def _generate_response_with_gemini(self, query: str, context: str, chat_history: Optional[List[Dict[str, str]]] = None) -> str:
        """Generate response using Gemini via OpenAI-compatible endpoint based on query and context"""
        try:
            # Format chat history for context
            history_text = ""
            if chat_history:
                for msg in chat_history[-5:]:  # Use last 5 messages as context
                    role = msg.get('role', 'user')
                    content = msg.get('content', '')
                    history_text += f"{role.capitalize()}: {content}\n"

            # Prepare the prompt for Gemini
            prompt = f"""
            You are an academic tutor specializing in Physical AI & Humanoid Robotics.
            Answer the following question based on the provided context in an academic, concise, and tutor-style manner.

            Conversation History:
            {history_text}

            Context: {context}

            Question: {query}

            Provide a clear, factual answer based only on the context provided.
            If the context doesn't contain the information needed to answer the question, respond with "I don't know".
            Do not hallucinate or make up information.
            """

            # Generate response using OpenAI client with Gemini endpoint
            response = await self.openai_client.chat.completions.create(
                model="gemini-pro",  # Using gemini-pro model through OpenAI-compatible endpoint
                messages=[
                    {"role": "system", "content": "You are an academic tutor specializing in Physical AI & Humanoid Robotics."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=1000
            )

            # Return the generated text
            return response.choices[0].message.content if response.choices[0].message.content else "I don't know"
        except Exception as e:
            logger.error(f"Error generating response with Gemini via OpenAI endpoint: {str(e)}")
            return "I don't know"

    async def process_query_stream(self, query: str, selected_text: Optional[str] = None,
                                  chat_history: Optional[List[Dict[str, str]]] = None,
                                  context: Optional[Dict[str, Any]] = None,
                                  conversation_id: Optional[str] = None) -> AsyncGenerator[str, None]:
        """Process a query and return response in streaming fashion"""
        try:
            # If selected_text exists, answer ONLY from selected_text
            if selected_text:
                logger.info("Using selected_text for streaming response generation")
                context_text = selected_text
                relevant_docs = []
            else:
                # Find relevant documents from Qdrant
                relevant_docs = self._find_relevant_documents(query)

                # If no relevant content found, respond with "I don't know"
                if not relevant_docs:
                    logger.info("No relevant content found in Qdrant")
                    response = "I don't know"
                    # Store user query and response in database if conversation_id provided
                    if conversation_id:
                        await self.db_service.add_message(conversation_id, "user", query)
                        await self.db_service.add_message(conversation_id, "assistant", response)
                    yield response
                    return

                # Create context from relevant documents
                context_text = "\n".join([doc.content for doc in relevant_docs])

            # If conversation_id is provided, get history from database
            if conversation_id:
                db_history = await self.db_service.get_conversation_history(conversation_id)
                # Combine with any provided history
                if chat_history:
                    full_history = db_history + chat_history
                else:
                    full_history = db_history
            else:
                full_history = chat_history or []

            # Format chat history for context
            history_text = ""
            if full_history:
                for msg in full_history[-5:]:  # Use last 5 messages as context
                    role = msg.get('role', 'user')
                    content = msg.get('content', '')
                    history_text += f"{role.capitalize()}: {content}\n"

            # Prepare the prompt for Gemini
            prompt = f"""
            You are an academic tutor specializing in Physical AI & Humanoid Robotics.
            Answer the following question based on the provided context in an academic, concise, and tutor-style manner.

            Conversation History:
            {history_text}

            Context: {context_text}

            Question: {query}

            Provide a clear, factual answer based only on the context provided.
            If the context doesn't contain the information needed to answer the question, respond with "I don't know".
            Do not hallucinate or make up information.
            """

            # Generate streaming response using OpenAI client with Gemini endpoint
            stream = await self.openai_client.chat.completions.create(
                model="gemini-pro",  # Using gemini-pro model through OpenAI-compatible endpoint
                messages=[
                    {"role": "system", "content": "You are an academic tutor specializing in Physical AI & Humanoid Robotics."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=1000,
                stream=True
            )

            full_response = ""
            # Stream the response
            async for chunk in stream:
                if chunk.choices and chunk.choices[0].delta and chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    full_response += content
                    yield content

            # Generate Urdu translation if needed
            urdu_translation = await self._translate_to_urdu(full_response) if full_response != "I don't know" and full_response != "I'm sorry, I encountered an error processing your request." else None

            # Store user query and response in database if conversation_id provided
            if conversation_id:
                await self.db_service.add_message(conversation_id, "user", query)
                await self.db_service.add_message(conversation_id, "assistant", full_response, urdu_translation)
        except Exception as e:
            logger.error(f"Error processing streaming query: {str(e)}")
            error_response = "I'm sorry, I encountered an error processing your request."
            # Store error response in database if conversation_id provided
            if conversation_id:
                await self.db_service.add_message(conversation_id, "user", query, None)
                await self.db_service.add_message(conversation_id, "assistant", error_response, None)
            yield error_response

    def health_check(self) -> bool:
        """Check if the RAG service is healthy"""
        try:
            # Test Qdrant connection by getting collection info
            collections = self.qdrant_client.get_collections()
            collection_names = [collection.name for collection in collections.collections]
            return self.collection_name in collection_names
        except:
            return False