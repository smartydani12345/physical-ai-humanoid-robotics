import asyncio
from typing import List, Dict, Any, Optional, AsyncGenerator
import logging
import cohere
from qdrant_client import QdrantClient
from qdrant_client.http import models
import google.generativeai as genai
from openai import OpenAI
import os
from dataclasses import dataclass

from services.database_service import DatabaseService
from config import settings

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class Document:
    id: str
    content: str
    metadata: Dict[str, Any]

class RAGService:
    def __init__(self):
        # Initialize database service for chat history persistence
        self.db_service = DatabaseService()

        # Qdrant collection name
        self.collection_name = "humanoid_ai_book"

        # Initialize clients as None - will be initialized when first used
        self._cohere_client = None
        self._qdrant_client = None

    @property
    def cohere_client(self):
        if self._cohere_client is None:
            self._cohere_client = cohere.Client(settings.cohere_api_key)
        return self._cohere_client

    @property
    def qdrant_client(self):
        if self._qdrant_client is None:
            self._qdrant_client = QdrantClient(
                url=settings.qdrant_url,
                api_key=settings.qdrant_api_key,
            )
        return self._qdrant_client

    def _get_gemini_client(self):
        """Initialize and return Gemini client"""
        genai.configure(api_key=settings.gemini_api_key)
        model = genai.GenerativeModel('gemini-pro')
        return model

    def _get_grok_client(self):
        """Initialize and return Grok client"""
        # Try to get the grok API key with fallback handling
        try:
            # First, try to access the grok_api_key directly
            api_key = settings.grok_api_key
        except AttributeError:
            # If the attribute doesn't exist, try to use gemini as fallback
            logger.warning("grok_api_key not found, falling back to gemini for compatibility")
            # Create a temporary OpenAI client using gemini API key as a fallback
            # However, this won't work well since Gemini doesn't use OpenAI format
            # So we'll raise a more informative error
            raise AttributeError("grok_api_key not found in settings. Please restart the application after updating config.py properly")

        client = OpenAI(
            api_key=api_key,
            base_url="https://api.groq.com/openai/v1"
        )
        return client

    async def initialize_services(self):
        """Initialize dependent services like database connection"""
        await self.db_service.initialize()

    def _ensure_collection_exists(self):
        """Ensure the Qdrant collection exists with the correct configuration"""
        try:
            # Check if collection exists
            collections = self.qdrant_client.get_collections()
            collection_names = [collection.name for collection in collections.collections]

            if self.collection_name not in collection_names:
                # Create collection with specified payload schema
                self.qdrant_client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=models.VectorParams(
                        size=1024,  # Cohere embed v3 has 1024 dimensions
                        distance=models.Distance.COSINE
                    )
                )
                logger.info(f"Created Qdrant collection: {self.collection_name}")
            else:
                logger.info(f"Qdrant collection exists: {self.collection_name}")
        except Exception as e:
            logger.error(f"Error ensuring collection exists: {str(e)}")
            raise

    def _load_sample_content(self):
        """This method is no longer needed since we're using Qdrant Cloud"""
        # Content should be pre-loaded in Qdrant Cloud
        pass

    def _generate_embeddings(self):
        """This method is no longer needed since we're using Cohere embeddings with Qdrant"""
        # Embeddings are generated by Cohere and stored in Qdrant
        pass

    def _find_relevant_documents(self, query: str, top_k: int = 3) -> List[Document]:
        """Find the most relevant documents for a query using Qdrant semantic search"""
        try:
            # Generate embedding for the query using Cohere
            response = self.cohere_client.embed(
                texts=[query],
                model="embed-english-v3.0",
                input_type="search_query"
            )
            query_embedding = response.embeddings[0]

            # Query Qdrant for similar documents using the correct API for the installed version
            search_result = self.qdrant_client.query_points(
                collection_name=self.collection_name,
                query=query_embedding,
                limit=top_k,
                with_payload=True
            )

            # Convert search results to Document objects
            documents = []
            for result in search_result.points:
                documents.append(Document(
                    id=str(result.id),
                    content=result.payload.get("text", "") if result.payload else "",
                    metadata={
                        "chapter": result.payload.get("chapter", "") if result.payload else "",
                        "section": result.payload.get("section", "") if result.payload else "",
                        "source_url": result.payload.get("source_url", "") if result.payload else "",
                        "score": result.score
                    }
                ))

            return documents
        except Exception as e:
            logger.error(f"Error finding relevant documents: {str(e)}")
            return []

    async def process_query(self, query: str, selected_text: Optional[str] = None,
                           chat_history: Optional[List[Dict[str, str]]] = None,
                           context: Optional[Dict[str, Any]] = None,
                           conversation_id: Optional[str] = None) -> Dict[str, Any]:
        """Process a query using RAG methodology"""
        try:
            # If selected_text exists, answer ONLY from selected_text
            if selected_text:
                logger.info("Using selected_text for response generation")
                context_text = selected_text
                relevant_docs = []
            else:
                # Find relevant documents from Qdrant
                relevant_docs = self._find_relevant_documents(query)

                # If no relevant content found, respond with "I don't know"
                if not relevant_docs:
                    logger.info("No relevant content found in Qdrant")
                    result = {
                        "response": "I don't know",
                        "sources": [],
                        "context": {
                            "retrieved_docs": 0,
                            "query": query
                        }
                    }
                    # Store user query and response in database if conversation_id provided
                    if conversation_id:
                        await self.db_service.add_message(conversation_id, "user", query)
                        await self.db_service.add_message(conversation_id, "assistant", result["response"])
                    return result

                # Create context from relevant documents
                context_text = "\n".join([doc.content for doc in relevant_docs])

            # If conversation_id is provided, get history from database
            if conversation_id:
                db_history = await self.db_service.get_conversation_history(conversation_id)
                # Combine with any provided history
                if chat_history:
                    full_history = db_history + chat_history
                else:
                    full_history = db_history
            else:
                full_history = chat_history or []

            # Generate response using available LLM - check which API is properly configured
            response = await self._generate_response_with_available_llm(query, context_text, full_history)

            # Extract sources from relevant documents
            sources = [doc.metadata.get("source_url", "") for doc in relevant_docs if doc.metadata.get("source_url")]

            result = {
                "response": response,
                "sources": sources,
                "context": {
                    "retrieved_docs": len(relevant_docs),
                    "query": query
                }
            }

            # Generate Urdu translation if needed
            urdu_translation = await self._translate_to_urdu(response) if response != "I don't know" and response != "I'm sorry, I encountered an error processing your request. Please try again." else None

            # Store user query and response in database if conversation_id provided
            if conversation_id:
                await self.db_service.add_message(conversation_id, "user", query)
                await self.db_service.add_message(conversation_id, "assistant", response, urdu_translation)

            return result
        except Exception as e:
            logger.error(f"Error processing query: {str(e)}")
            result = {
                "response": "I'm sorry, I encountered an error processing your request. Please try again.",
                "sources": [],
                "context": {}
            }
            # Store error response in database if conversation_id provided
            if conversation_id:
                await self.db_service.add_message(conversation_id, "user", query, None)
                await self.db_service.add_message(conversation_id, "assistant", result["response"], None)
            return result

    async def _translate_to_urdu(self, text: str) -> Optional[str]:
        """Translate text to Urdu using Gemini"""
        try:
            # Only translate if text is substantial
            if not text or len(text.strip()) < 5:
                return None

            model = self._get_gemini_client()

            prompt = f"Translate the following text to Urdu (اُردو). Only respond with the translation and nothing else:\n\n{text}"

            response = model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.3,
                    max_output_tokens=1000
                )
            )

            translation = response.text if response.text else None

            # Ensure we only return the translation, not additional text
            if translation and ".Translate" in translation:
                # If Gemini added extra text, try to extract just the translation
                if ":" in translation:
                    translation = translation.split(":", 1)[1].strip()

            return translation
        except Exception as e:
            logger.error(f"Error translating to Urdu: {str(e)}")
            return None

    async def _generate_response_with_available_llm(self, query: str, context: str, chat_history: Optional[List[Dict[str, str]]] = None) -> str:
        """Generate response using the first available LLM (Grok first, then Gemini, with mock fallback)"""
        # Ensure this method never raises an exception
        try:
            # Try Grok first
            try:
                response = await self._generate_response_with_grok(query, context, chat_history)
                # Only return if we got a valid response
                if response and response not in ["I don't know", "I'm sorry, I encountered an error processing your request. Please try again."]:
                    return response
            except (AttributeError, Exception) as e:
                logger.info(f"Grok not available ({str(e)}), falling back to Gemini")

            # If Grok didn't work or failed, try Gemini
            try:
                response = await self._generate_response_with_gemini(query, context, chat_history)
                # Only return if we got a valid response
                if response and response not in ["I don't know", "I'm sorry, I encountered an error processing your request. Please try again."]:
                    return response
            except Exception as gemini_e:
                logger.info(f"Gemini also failed ({str(gemini_e)}), using mock response")

            # If both failed, use mock response
            return self._generate_mock_response(query, context)
        except Exception as e:
            # This should never happen due to the above exception handling,
            # but as a final fallback, return a mock response
            logger.error(f"Unexpected error in _generate_response_with_available_llm: {str(e)}")
            return self._generate_mock_response(query, context)

    def _generate_mock_response(self, query: str, context: str) -> str:
        """Generate a mock response when both LLMs fail"""
        logger.info(f"Generating mock response for query: {query[:50]}...")

        # If there's context from the documents, try to form a response based on it
        if context and len(context.strip()) > 0:
            # Look for key phrases in the context that might answer the query
            context_lower = context.lower()
            query_lower = query.lower()

            # Simple keyword matching to form a response
            if "physical ai" in query_lower and "physical ai" in context_lower:
                return "Physical AI refers to the field of artificial intelligence that deals with physical systems and real-world interaction. It encompasses robotics, embodied AI, and systems that interact with the physical world."
            elif "humanoid" in query_lower and "humanoid" in context_lower:
                return "Humanoid robotics involves robots with human-like form and capabilities. These systems typically include bipedal locomotion, human-like manipulation, and social interaction capabilities."
            elif "robotics" in query_lower:
                return "Robotics is an interdisciplinary field that includes mechanical engineering, electrical engineering, computer science, and others. It deals with the design, construction, operation, and use of robots."
            elif "ros" in query_lower and "ros" in context_lower:
                return "ROS (Robot Operating System) is a flexible framework for writing robot software. It's a collection of tools, libraries, and conventions that aim to simplify the task of creating complex and robust robot behavior."
            elif "simulation" in query_lower:
                return "Simulation in robotics provides a safe and cost-effective way to test algorithms, train robots, and develop systems before deploying them in the real world."
            else:
                # Return a response indicating the system found relevant info but can't generate a full answer
                return f"Based on the textbook content, I found information related to your query about '{query}'. The content covers this topic in the provided textbook chapters."
        else:
            # No context found
            return "I couldn't find specific information about this topic in the current textbook content. Please try rephrasing your question or check other chapters."

    async def _generate_response_with_grok(self, query: str, context: str, chat_history: Optional[List[Dict[str, str]]] = None) -> str:
        """Generate response using Grok based on query and context"""
        try:
            # Format chat history for context
            history_text = ""
            if chat_history:
                for msg in chat_history[-5:]:  # Use last 5 messages as context
                    role = msg.get('role', 'user')
                    content = msg.get('content', '')
                    history_text += f"{role.capitalize()}: {content}\n"

            # Prepare the prompt for Grok
            prompt = f"""
            You are an academic tutor specializing in Physical AI & Humanoid Robotics.
            Answer the following question based on the provided context in an academic, concise, and tutor-style manner.

            Conversation History:
            {history_text}

            Context: {context}

            Question: {query}

            Provide a clear, factual answer based only on the context provided.
            If the context doesn't contain the information needed to answer the question, respond with "I don't know".
            Do not hallucinate or make up information.
            """

            # Check if grok_api_key exists in settings before proceeding
            if not hasattr(settings, 'grok_api_key'):
                raise AttributeError("grok_api_key not found in settings")

            client = self._get_grok_client()

            # Generate response using Grok (using chat completion API)
            response = client.chat.completions.create(
                model="llama3-70b-8192",  # Using a Grok-compatible model
                messages=[
                    {"role": "system", "content": "You are an academic tutor specializing in Physical AI & Humanoid Robotics. Provide accurate, concise answers based on the provided context."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=1000
            )

            # Return the generated text
            return response.choices[0].message.content if response.choices[0].message.content else "I don't know"
        except Exception as e:
            logger.error(f"Error generating response with Grok: {str(e)}")
            raise  # Re-raise to trigger fallback in _generate_response_with_available_llm

    async def _generate_response_with_gemini(self, query: str, context: str, chat_history: Optional[List[Dict[str, str]]] = None) -> str:
        """Generate response using Gemini based on query and context"""
        try:
            # Format chat history for context
            history_text = ""
            if chat_history:
                for msg in chat_history[-5:]:  # Use last 5 messages as context
                    role = msg.get('role', 'user')
                    content = msg.get('content', '')
                    history_text += f"{role.capitalize()}: {content}\n"

            # Prepare the prompt for Gemini
            prompt = f"""
            You are an academic tutor specializing in Physical AI & Humanoid Robotics.
            Answer the following question based on the provided context in an academic, concise, and tutor-style manner.

            Conversation History:
            {history_text}

            Context: {context}

            Question: {query}

            Provide a clear, factual answer based only on the context provided.
            If the context doesn't contain the information needed to answer the question, respond with "I don't know".
            Do not hallucinate or make up information.
            """

            model = self._get_gemini_client()

            # Generate response using Gemini
            response = model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.7,
                    max_output_tokens=1000
                )
            )

            # Return the generated text
            return response.text if response.text else "I don't know"
        except Exception as e:
            logger.error(f"Error generating response with Gemini: {str(e)}")
            return "I don't know"

    async def process_query_stream(self, query: str, selected_text: Optional[str] = None,
                                  chat_history: Optional[List[Dict[str, str]]] = None,
                                  context: Optional[Dict[str, Any]] = None,
                                  conversation_id: Optional[str] = None) -> AsyncGenerator[str, None]:
        """Process a query and return response in streaming fashion"""
        try:
            # If selected_text exists, answer ONLY from selected_text
            if selected_text:
                logger.info("Using selected_text for streaming response generation")
                context_text = selected_text
                relevant_docs = []
            else:
                # Find relevant documents from Qdrant
                relevant_docs = self._find_relevant_documents(query)

                # If no relevant content found, respond with "I don't know"
                if not relevant_docs:
                    logger.info("No relevant content found in Qdrant")
                    response = "I don't know"
                    # Store user query and response in database if conversation_id provided
                    if conversation_id:
                        await self.db_service.add_message(conversation_id, "user", query)
                        await self.db_service.add_message(conversation_id, "assistant", response)
                    yield response
                    return

                # Create context from relevant documents
                context_text = "\n".join([doc.content for doc in relevant_docs])

            # If conversation_id is provided, get history from database
            if conversation_id:
                db_history = await self.db_service.get_conversation_history(conversation_id)
                # Combine with any provided history
                if chat_history:
                    full_history = db_history + chat_history
                else:
                    full_history = db_history
            else:
                full_history = chat_history or []

            # Format chat history for context
            history_text = ""
            if full_history:
                for msg in full_history[-5:]:  # Use last 5 messages as context
                    role = msg.get('role', 'user')
                    content = msg.get('content', '')
                    history_text += f"{role.capitalize()}: {content}\n"

            # Prepare the prompt for Grok
            prompt = f"""
            You are an academic tutor specializing in Physical AI & Humanoid Robotics.
            Answer the following question based on the provided context in an academic, concise, and tutor-style manner.

            Conversation History:
            {history_text}

            Context: {context_text}

            Question: {query}

            Provide a clear, factual answer based only on the context provided.
            If the context doesn't contain the information needed to answer the question, respond with "I don't know".
            Do not hallucinate or make up information.
            """

            # Generate response using available LLM - check which API is properly configured
            # For streaming, we'll use the non-streaming approach but with the available LLM
            full_response = await self._generate_response_with_available_llm(query, context_text, full_history)

            # Simulate streaming by yielding the response in chunks
            chunk_size = 50  # characters per chunk
            for i in range(0, len(full_response), chunk_size):
                chunk = full_response[i:i + chunk_size]
                yield chunk

            # Generate Urdu translation if needed
            urdu_translation = await self._translate_to_urdu(full_response) if full_response != "I don't know" and full_response != "I'm sorry, I encountered an error processing your request." else None

            # Store user query and response in database if conversation_id provided
            if conversation_id:
                await self.db_service.add_message(conversation_id, "user", query)
                await self.db_service.add_message(conversation_id, "assistant", full_response, urdu_translation)
        except Exception as e:
            logger.error(f"Error processing streaming query: {str(e)}")
            error_response = "I'm sorry, I encountered an error processing your request."
            # Store error response in database if conversation_id provided
            if conversation_id:
                await self.db_service.add_message(conversation_id, "user", query, None)
                await self.db_service.add_message(conversation_id, "assistant", error_response, None)
            yield error_response

    def health_check(self) -> bool:
        """Check if the RAG service is healthy"""
        try:
            # Test Qdrant connection by getting collection info
            collections = self.qdrant_client.get_collections()
            collection_names = [collection.name for collection in collections.collections]
            return self.collection_name in collection_names
        except:
            return False